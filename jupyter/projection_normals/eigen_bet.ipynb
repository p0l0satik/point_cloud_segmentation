{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laserscan import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flaten_3x3(m):\n",
    "     m = np.pad(m, [(1, 1), (1, 1)], mode='constant')\n",
    "     arr = np.array([np.matrix([m[i][j:j+3], m[i+1][j:j+3], m[i+2][j:j+3]]).flatten(\"C\") for j in range(len(m[0])-2) for i in range(len(m)-2)])\n",
    "     arr_s = np.squeeze(arr).reshape((len(m)-2, len(m[0])-2, 9))\n",
    "     return np.transpose(arr_s, axes=(1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700\n",
      "300\n",
      "points shape:  (64, 1024, 3)\n",
      "shape x :  (1024, 64, 9)\n",
      "torch.Size([1, 1024, 64, 3, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f135d92df40>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x5760 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from laserscan import *\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "device = \"cuda:0\"\n",
    "# device=\"cpu\"\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class CustomKitti(Dataset):\n",
    "    def __init__(self, dir, mode=\"train\"):\n",
    "        self.ls = SemLaserScan(project=True, nclasses=100)\n",
    "        self.mode = mode\n",
    "        self.len = 4541\n",
    "        self.train_len = 3700\n",
    "        self.val_len = 300\n",
    "        self.test_len = self.len - self.train_len - self.val_len\n",
    "        self.label_folder = dir + \"plane_labels/\"\n",
    "        self.file_folder = dir + \"velodyne/\"\n",
    "    def __len__(self):\n",
    "        if self.mode == \"train\":\n",
    "            return self.train_len\n",
    "        if self.mode == \"val\":\n",
    "            return self.val_len\n",
    "        return self.test_len\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            idx += self.train_len + self.val_len\n",
    "        if self.mode == \"val\":\n",
    "            idx += self.train_len\n",
    "        self.ls.open_scan(self.file_folder + '{0:06d}.bin'.format(idx))\n",
    "        self.ls.open_label(self.label_folder + 'label-{0:06d}.npy'.format(idx))\n",
    "        self.ls.proj_sem_label[self.ls.proj_sem_label != 0] = 1\n",
    "        print(\"points shape: \", self.ls.proj_xyz.shape)\n",
    "        print(\"shape x : \", flaten_3x3(self.ls.proj_xyz[:,:, 0]).shape)\n",
    "        return np.stack((flaten_3x3(self.ls.proj_xyz[:,:, 0]),flaten_3x3(self.ls.proj_xyz[:,:, 1]), flaten_3x3(self.ls.proj_xyz[:,:, 2])), axis=2).squeeze(), self.ls.proj_sem_label\n",
    "\n",
    "training_data = CustomKitti(\"/home/polosatik/mnt/kitty/dataset/sequences/00/\") \n",
    "validation_data = CustomKitti(\"/home/polosatik/mnt/kitty/dataset/sequences/00/\", mode=\"val\") \n",
    "test_data = CustomKitti(\"/home/polosatik/mnt/kitty/dataset/sequences/00/\", mode=\"test\") \n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "training_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "print(len(training_loader))\n",
    "print(len(validation_loader))\n",
    "\n",
    "vinputs, vlabels = next(iter(test_loader))\n",
    "print(vinputs.shape)\n",
    "plt.figure(figsize=(20,80))\n",
    "plt.imshow(vlabels[0], cmap=\"gray\")\n",
    "# validation_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):\n",
    "    # Average of Dice coefficient for all batches, or for a single mask\n",
    "    assert input.size() == target.size()\n",
    "    if input.dim() == 2 and reduce_batch_first:\n",
    "        raise ValueError(f'Dice: asked to reduce batch but got tensor without batch dimension (shape {input.shape})')\n",
    "\n",
    "    if input.dim() == 2 or reduce_batch_first:\n",
    "        inter = torch.dot(input.reshape(-1), target.reshape(-1))\n",
    "        sets_sum = torch.sum(input) + torch.sum(target)\n",
    "        if sets_sum.item() == 0:\n",
    "            sets_sum = 2 * inter\n",
    "\n",
    "        return (2 * inter + epsilon) / (sets_sum + epsilon)\n",
    "    else:\n",
    "        # compute and average metric for each batch element\n",
    "        dice = 0\n",
    "        for i in range(input.shape[0]):\n",
    "            dice += dice_coeff(input[i, ...], target[i, ...])\n",
    "        return dice / input.shape[0]\n",
    "\n",
    "\n",
    "def multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):\n",
    "    # Average of Dice coefficient for all classes\n",
    "    assert input.size() == target.size()\n",
    "    dice = 0\n",
    "    for channel in range(input.shape[1]):\n",
    "        dice += dice_coeff(input[:, channel, ...], target[:, channel, ...], reduce_batch_first, epsilon)\n",
    "\n",
    "    return dice / input.shape[1]\n",
    "\n",
    "\n",
    "def dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):\n",
    "    # Dice loss (objective to minimize) between 0 and 1\n",
    "    assert input.size() == target.size()\n",
    "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
    "    return 1 - fn(input, target, reduce_batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch-vectorized\n",
    "import torch\n",
    "from torchvectorized.utils import sym\n",
    "from torchvectorized.vlinalg import vSymEig\n",
    "from torchvectorized.nn import EigVals\n",
    "\n",
    "# Random batch of volumetric 3x3 symmetric matrices of size 16x9x32x32x32\n",
    "input = sym(torch.rand(4, 9, 3, 1024, 64))\n",
    "\n",
    "# Output eig_vals with size: 16x3x32x32x32 and eig_vecs with size 16,3,3,32,32,32\n",
    "eig_vals, eig_vecs = vSymEig(input, eigenvectors=True)\n",
    "# eig_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4, 2, 5]) torch.Size([1, 3, 3, 4, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvectorized.utils import sym\n",
    "from torchvectorized.vlinalg import vSymEig\n",
    "\n",
    "b, c, d, h, w = 1, 9, 4, 2, 5\n",
    "inputs = sym(torch.rand(b, c, d, h, w))\n",
    "v, u = vSymEig(inputs, eigenvectors=True)\n",
    "print(v.shape, u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvectorized.vlinalg import vSymEig\n",
    "\n",
    "\n",
    "def _grad_sym(X):\n",
    "    return 0.5 * (X + X.transpose(1, 2))\n",
    "\n",
    "def floor_div(input, K):\n",
    "    rem = torch.remainder(input, K)\n",
    "    out = (input - rem) / K\n",
    "    return out\n",
    "\n",
    "def normal(m):\n",
    "    p = torch.sum(torch.sum(m, axis=1), axis=0)\n",
    "    arr = (m.permute(1, 2, 0).flatten(0, 1) - floor_div(p, 9)).reshape(9,3,1)\n",
    "    cov_m = torch.zeros(3, 3).to(device)\n",
    "    for vec in arr:\n",
    "        cov_m += vec@vec.T\n",
    "    cov_m/9\n",
    "    L, V = torch.linalg.eig(cov_m/9)\n",
    "    L, V = torch.real(L),torch.real(V)\n",
    "    return V[torch.argmin(L)]\n",
    "\n",
    "class EigValsFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X):\n",
    "        # torch.Size([1, 1024, 64, 3, 9])\n",
    "        print(\"X \", X.shape)\n",
    "        X = X.reshape(1024*64, 3, 3, 3)\n",
    "\n",
    "        tList = [normal(a) for a in torch.unbind(X, dim=0) ]\n",
    "        res = torch.stack(tList, dim=0).reshape(1, 1024, 64, 3)\n",
    "        print(\"res \", res.shape)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        grad_input = grad_outputs.clone()\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class EigVals(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Differentiable neural network layer (:class:`torch.nn.Module`) that performs eigendecomposition on\n",
    "    every voxel in a volume of flattened 3x3 symmetric matrices of shape **Bx9xDxHxW** and return the eigenvalues.\n",
    "\n",
    "    See **Ionescu et al., Matrix backpropagation for deep networks with structured layers, CVPR 2015** for details on the\n",
    "    gradients computation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "                                                                    ([1, 9, 3, 64, 1024])\n",
    "        Takes a volume of flattened 3x3 symmetric matrices of shape **Bx9xDxHxW** and return a volume of their eigenvalues\n",
    "\n",
    "        :param x: A volume of flattened 3x3 symmetric matrices of shape **Bx9xDxHxW**\n",
    "        :type x: torch.Tensor\n",
    "        :return: A tensor with shape **(B*D*H*W)x3** where every voxel's channels are the eigenvalues of the inpur matrix\n",
    "                                    ([1, 3, 64, 3, 9])\n",
    "            at the same spatial location.\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        X = x.reshape(1024*64, 3, 3, 3)\n",
    "\n",
    "        tList = [normal(a) for a in torch.unbind(X, dim=0) ]\n",
    "        res = torch.stack(tList, dim=0).reshape(1, 1024, 64, 3)\n",
    "        print(\"res \", res.shape)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_classes = 2\n",
    "from unet import UNet\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# import torchvectorized\n",
    "def get_model_and_optimizer(device, num_encoding_blocks=5, out_channels_first_layer=8, patience=3):\n",
    "    #Better to train with num_encoding_blocks >=3, out_channels_first_layer>=4 '''\n",
    "    #repoducibility\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "      \n",
    "    # unet = UNet(\n",
    "    #       in_channels=5,\n",
    "    #       out_classes=max_classes,\n",
    "    #       dimensions=2,\n",
    "    #       num_encoding_blocks=num_encoding_blocks,\n",
    "    #       normalization='batch',\n",
    "    #       upsampling_type='linear',\n",
    "    #       padding=True,\n",
    "    #       activation='ReLU',\n",
    "    #   ).to(device)\n",
    "    # print( \"unet par:\", unet.parameters())\n",
    "    # model = unet\n",
    "    EV = EigVals()\n",
    "    # print(\"par: \", list(EV.parameters()))\n",
    "    model = nn.Sequential(nn.Linear(in_features=9, out_features=9, bias=False), EV, nn.Linear(3, 1, bias=False), nn.Softmax())\n",
    "    model.to(device=device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=patience, threshold=0.01)\n",
    "    \n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "def train_one_epoch(epoch_index, tb_writer, optimizer, model):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = torch.tensor(inputs).to(device=device, dtype=torch.float)\n",
    "        # print(inputs.shape)\n",
    "        labels = torch.tensor(labels).to(device=device, dtype=torch.float)\n",
    "        # print(torch.max(labels))\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs).squeeze(dim=3)\n",
    "\n",
    "        # print(outputs.shape, labels.shape)\n",
    "        # outputs = outputs.permute(0, 2, 1)\n",
    "        # print(outputs.shape, labels.shape)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = torch.nn.functional.mse_loss(outputs.permute(0, 2, 1), labels)\n",
    "\n",
    "        # loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 100== 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            # tb_writer.add_image(\"label\", visualize_seg(labels.numpy(), mc).squeeze()[0], 0)\n",
    "            # tb_writer.add_image(\"label\", visualize_seg(outputs.cpu().numpy(), mc).squeeze()[0], 0)\n",
    "\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# run_name = \"SEMANTIC_log softmax_nll_\"\n",
    "run_name = \"SEMANTIC_cross_entropy_dice_softmax_5 s\"\n",
    "writer = SummaryWriter('runs/{}_{}'.format(run_name, timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 4\n",
    "\n",
    "best_vloss = 1_000_00000.\n",
    "model, optimizer, scheduler = get_model_and_optimizer(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "points shape:  (64, 1024, 3)\n",
      "shape x :  (1024, 64, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12549/1402769369.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs).to(device=device, dtype=torch.float)\n",
      "/tmp/ipykernel_12549/1402769369.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device=device, dtype=torch.float)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m avg_loss \u001b[39m=\u001b[39m train_one_epoch(epoch_number, writer, optimizer, model)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# We don't need gradients on to do reporting\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb Cell 12\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, tb_writer, optimizer, model)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Make predictions for this batch\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\u001b[39m.\u001b[39msqueeze(dim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# print(outputs.shape, labels.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# outputs = outputs.permute(0, 2, 1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# print(outputs.shape, labels.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Compute the loss and its gradients\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mmse_loss(outputs\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m), labels)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb Cell 12\u001b[0m in \u001b[0;36mEigVals.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m                                                            ([1, 9, 3, 64, 1024])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mTakes a volume of flattened 3x3 symmetric matrices of shape **Bx9xDxHxW** and return a volume of their eigenvalues\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m:rtype: torch.Tensor\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m X \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(\u001b[39m1024\u001b[39m\u001b[39m*\u001b[39m\u001b[39m64\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m tList \u001b[39m=\u001b[39m [normal(a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m torch\u001b[39m.\u001b[39munbind(X, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) ]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(tList, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m1024\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mres \u001b[39m\u001b[39m\"\u001b[39m, res\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;32m/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb Cell 12\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m                                                            ([1, 9, 3, 64, 1024])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mTakes a volume of flattened 3x3 symmetric matrices of shape **Bx9xDxHxW** and return a volume of their eigenvalues\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m:rtype: torch.Tensor\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m X \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(\u001b[39m1024\u001b[39m\u001b[39m*\u001b[39m\u001b[39m64\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m tList \u001b[39m=\u001b[39m [normal(a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m torch\u001b[39m.\u001b[39munbind(X, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) ]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(tList, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m1024\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mres \u001b[39m\u001b[39m\"\u001b[39m, res\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;32m/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb Cell 12\u001b[0m in \u001b[0;36mnormal\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m cov_m\u001b[39m/\u001b[39m\u001b[39m9\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m L, V \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39meig(cov_m\u001b[39m/\u001b[39m\u001b[39m9\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m L, V \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mreal(L),torch\u001b[39m.\u001b[39mreal(V)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B31.135.65.84/home/polosatik/SqueezeSeg/src/eigen_bet.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mreturn\u001b[39;00m V[torch\u001b[39m.\u001b[39margmin(L)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = True\n",
    "if train :\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss = train_one_epoch(epoch_number, writer, optimizer, model)\n",
    "\n",
    "        with torch.no_grad():\n",
    "        # We don't need gradients on to do reporting\n",
    "            model.train(False)\n",
    "\n",
    "            running_vloss = 0.0\n",
    "            for i, vdata in enumerate(validation_loader):\n",
    "                vinputs, vlabels = vdata\n",
    "                vinputs = torch.tensor(vinputs).to(device=device, dtype=torch.float)\n",
    "                vlabels = torch.tensor(vlabels).to(device=device, dtype=torch.float)\n",
    "\n",
    "                voutputs = model(vinputs).squeeze(dim=3)\n",
    "                vloss = torch.nn.functional.mse_loss(voutputs.permute(0, 2, 1), vlabels)\n",
    "                running_vloss += vloss\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "        # Log the running loss averaged per batch\n",
    "        # for both training and validation\n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                        { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                        epoch_number + 1)\n",
    "        writer.flush()\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = 'chpt/model_{}_{}_{}'.format(run_name, timestamp, epoch_number)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196608\n",
      "192.0\n",
      "1728\n"
     ]
    }
   ],
   "source": [
    "print(64*1024*3)\n",
    "print(1728/9)\n",
    "print(64*3*9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9061)\n",
      "torch.Size([64, 1024]) torch.Size([2, 64, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9218953322843892, 0.9256186629880894, 0.92189533192352, 0.9293721914617133)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchmetrics import JaccardIndex\n",
    "import metrics\n",
    "\n",
    "jaccard = JaccardIndex(num_classes=2)\n",
    "pred = F.softmax(voutputs, dim=1)[0].permute(1,2,0).detach().squeeze()[:,:, 1]\n",
    "vlabels.shape\n",
    "print(jaccard(pred.cpu(), vlabels[0].int().cpu()))\n",
    "metric_calculator = metrics.SegmentationMetrics(average=True, ignore_background=True, activation=\"softmax\")\n",
    "print(vlabels[0].int().shape, voutputs[0].shape)\n",
    "metric_calculator(vlabels.int(), voutputs)\n",
    "# pred\n",
    "# F.softmax(voutputs, dim=1)[0].cpu().permute(1,2,0).detach().numpy().squeeze()[:,:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14367/1610888461.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tinputs = torch.tensor(tinputs).permute(0,3,1,2).to(device=device, dtype=torch.float)\n",
      "/tmp/ipykernel_14367/1610888461.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tlabels = torch.tensor(tlabels).to(device=device, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference: 64.68943786621094, batch_iou: 0.8447953462600708, precision: 0.7793742083436951, recall: 0.8479480097156241, dice: 0.8122162917723272, acc: 0.7793742088654328\n",
      "inference: 59.759742736816406, batch_iou: 0.8830991983413696, precision: 0.847964815312653, recall: 0.8930762186791857, dice: 0.8699360853724435, acc: 0.8479648159053155\n",
      "inference: 58.6673583984375, batch_iou: 0.9214291572570801, precision: 0.9258089691020912, recall: 0.9449404892353417, dice: 0.935276903374103, acc: 0.9258089694196798\n",
      "inference: 56.51443099975586, batch_iou: 0.8905607461929321, precision: 0.904352996881189, recall: 0.9082917699426406, dice: 0.9063181040228906, acc: 0.9043529973648238\n",
      "inference: 56.59910583496094, batch_iou: 0.905887246131897, precision: 0.8841247129011479, recall: 0.927893137910955, dice: 0.905480322684691, acc: 0.8841247134371755\n",
      "inference: 56.567134857177734, batch_iou: 0.7872521281242371, precision: 0.553210848388902, recall: 0.8677899469837331, dice: 0.6756798649185617, acc: 0.5532108486285355\n",
      "inference: 56.63711929321289, batch_iou: 0.8604093790054321, precision: 0.8313475645053379, recall: 0.8343338625372185, dice: 0.8328380364862088, acc: 0.8313475650991954\n",
      "inference: 56.536705017089844, batch_iou: 0.8427658081054688, precision: 0.8640434757358865, recall: 0.7669203227739405, dice: 0.8125900793866195, acc: 0.8640434763151927\n",
      "inference: 56.66041564941406, batch_iou: 0.8879827857017517, precision: 0.8684469939276822, recall: 0.8862654198659136, dice: 0.8772657372403554, acc: 0.8684469945486138\n",
      "inference: 56.521793365478516, batch_iou: 0.8669235706329346, precision: 0.8106472924073891, recall: 0.8532580502141522, dice: 0.8314070642868573, acc: 0.8106472929800872\n",
      "inference: 56.8026237487793, batch_iou: 0.8340128660202026, precision: 0.6416510926425486, recall: 0.8924464414805342, dice: 0.746548666559705, acc: 0.6416510930106443\n",
      "inference: 56.73984146118164, batch_iou: 0.8115527033805847, precision: 0.8080828836193336, recall: 0.7441803580815185, dice: 0.774816272750449, acc: 0.8080828842007879\n",
      "inference: 56.79024124145508, batch_iou: 0.8755879998207092, precision: 0.8280499989948485, recall: 0.8970635655127894, dice: 0.8611763303664286, acc: 0.8280499996220299\n",
      "inference: 56.785377502441406, batch_iou: 0.800683856010437, precision: 0.6922248292225986, recall: 0.8588278220786533, dice: 0.7665786740838897, acc: 0.6922248294684852\n",
      "inference: 56.94617462158203, batch_iou: 0.8062745928764343, precision: 0.7423341685520108, recall: 0.7782121482021861, dice: 0.7598498796586219, acc: 0.742334169284665\n",
      "inference: 56.781822204589844, batch_iou: 0.8356263637542725, precision: 0.77407026474743, recall: 0.8277347144292249, dice: 0.8000035432296748, acc: 0.7740702655585965\n",
      "inference: 56.90969467163086, batch_iou: 0.8641153573989868, precision: 0.8699163913930877, recall: 0.892570821035518, dice: 0.8810980099042337, acc: 0.8699163917493367\n",
      "inference: 56.9317741394043, batch_iou: 0.8429806232452393, precision: 0.8681465605317767, recall: 0.8873852867878388, dice: 0.8776605058405188, acc: 0.868146560892103\n",
      "inference: 56.776798248291016, batch_iou: 0.8527064323425293, precision: 0.8756158548549194, recall: 0.8782593928132088, dice: 0.8769356315620024, acc: 0.8756158552615051\n",
      "inference: 56.830623626708984, batch_iou: 0.8586559891700745, precision: 0.8968171828886802, recall: 0.9029211987491926, dice: 0.8998588395671279, acc: 0.8968171832890371\n",
      "inference: 56.949951171875, batch_iou: 0.8930860161781311, precision: 0.8963273135312354, recall: 0.8743749199900425, dice: 0.8852150386361178, acc: 0.8963273139592756\n",
      "inference: 56.77833557128906, batch_iou: 0.8213704824447632, precision: 0.8486718470091147, recall: 0.6978000155556471, dice: 0.7658764990195623, acc: 0.8486718478186549\n",
      "inference: 57.00041580200195, batch_iou: 0.8873710632324219, precision: 0.9149921167935086, recall: 0.8629111192391715, dice: 0.888188800794578, acc: 0.9149921173050912\n",
      "inference: 56.855712890625, batch_iou: 0.8623924851417542, precision: 0.8872321199201628, recall: 0.811636120977896, dice: 0.8477521903449299, acc: 0.8872321207044025\n",
      "inference: 56.74854278564453, batch_iou: 0.8760720491409302, precision: 0.8793128889611087, recall: 0.8308228551248465, dice: 0.8543804168054059, acc: 0.8793128895680564\n",
      "inference: 56.762718200683594, batch_iou: 0.7739900350570679, precision: 0.6795238268317492, recall: 0.7177184694254201, dice: 0.6980991087479459, acc: 0.6795238272755387\n",
      "inference: 56.70275115966797, batch_iou: 0.8613432049751282, precision: 0.7922148389806112, recall: 0.9311582071576222, dice: 0.8560855129412582, acc: 0.7922148396031902\n",
      "inference: 56.748191833496094, batch_iou: 0.8187260627746582, precision: 0.7392026007572491, recall: 0.8233938296219679, dice: 0.7790301429194003, acc: 0.7392026012626332\n",
      "inference: 56.8421745300293, batch_iou: 0.8564853668212891, precision: 0.841267019245485, recall: 0.9581905795436751, dice: 0.895930121663255, acc: 0.841267019520358\n",
      "inference: 56.82166290283203, batch_iou: 0.8279561400413513, precision: 0.8582874082297557, recall: 0.8695607515400468, dice: 0.8638873033910102, acc: 0.8582874086836807\n",
      "inference: 56.789825439453125, batch_iou: 0.9123630523681641, precision: 0.9079896697478362, recall: 0.9234340661509717, dice: 0.9156467466338751, acc: 0.9079896701306851\n",
      "inference: 56.72380828857422, batch_iou: 0.8959693908691406, precision: 0.8416584489620083, recall: 0.9119282637365211, dice: 0.875385428524973, acc: 0.8416584496352064\n",
      "inference: 56.91958236694336, batch_iou: 0.8261114358901978, precision: 0.8084262914241513, recall: 0.8131428322858457, dice: 0.8107777024683331, acc: 0.80842629190109\n",
      "inference: 56.79926300048828, batch_iou: 0.8790826201438904, precision: 0.8716713062449399, recall: 0.9522645472494589, dice: 0.9101873623315089, acc: 0.8716713065448681\n",
      "inference: 56.75843048095703, batch_iou: 0.8837357759475708, precision: 0.8509558085161107, recall: 0.9009774131735495, dice: 0.8752524965613123, acc: 0.8509558090546108\n",
      "inference: 56.94438552856445, batch_iou: 0.8022012114524841, precision: 0.7536640249002942, recall: 0.7703237272736355, dice: 0.7619028169757598, acc: 0.7536640254287952\n",
      "inference: 56.80748748779297, batch_iou: 0.8406193256378174, precision: 0.726990653676113, recall: 0.8234473126170182, dice: 0.7722185769597268, acc: 0.7269906542162398\n",
      "inference: 56.79478454589844, batch_iou: 0.8957964181900024, precision: 0.9040406248854189, recall: 0.917908379351433, dice: 0.9109217249372994, acc: 0.9040406251688539\n",
      "inference: 56.8218879699707, batch_iou: 0.9230517148971558, precision: 0.9328796530977347, recall: 0.9384656665817778, dice: 0.9356643226313829, acc: 0.932879653446105\n",
      "inference: 56.863487243652344, batch_iou: 0.921891450881958, precision: 0.9227035078850443, recall: 0.9097930909508923, dice: 0.9162028207751727, acc: 0.9227035081941024\n",
      "inference: 56.87654495239258, batch_iou: 0.8227270841598511, precision: 0.7693088359020199, recall: 0.7738811726955943, dice: 0.7715882305181654, acc: 0.7693088363317107\n",
      "inference: 56.814273834228516, batch_iou: 0.8435490131378174, precision: 0.7712602830751756, recall: 0.8600878825157221, dice: 0.8132557324912425, acc: 0.7712602836578264\n",
      "inference: 56.77596664428711, batch_iou: 0.8349423408508301, precision: 0.7174449956952157, recall: 0.8589192436055137, dice: 0.7818336620099356, acc: 0.7174449964157558\n",
      "inference: 56.7716178894043, batch_iou: 0.8953830003738403, precision: 0.8832074033253798, recall: 0.8658011028039907, dice: 0.8744176384357121, acc: 0.883207403925742\n",
      "inference: 56.9510383605957, batch_iou: 0.9160639047622681, precision: 0.888237192818575, recall: 0.9337862433895329, dice: 0.9104423741344332, acc: 0.8882371933704071\n",
      "inference: 56.76502227783203, batch_iou: 0.915911078453064, precision: 0.8997811723445681, recall: 0.9278656097497333, dice: 0.913607612014624, acc: 0.8997811728189071\n",
      "inference: 56.83555221557617, batch_iou: 0.8793853521347046, precision: 0.872730067884264, recall: 0.8982435117498734, dice: 0.8853030106965386, acc: 0.8727300683756247\n",
      "inference: 56.89686584472656, batch_iou: 0.9141655564308167, precision: 0.934107633575129, recall: 0.9446112251429198, dice: 0.9393300674695894, acc: 0.9341076338603338\n",
      "inference: 56.7658576965332, batch_iou: 0.9173678159713745, precision: 0.912938791079543, recall: 0.9217509042109494, dice: 0.9173236850901477, acc: 0.9129387917126109\n",
      "inference: 56.88185501098633, batch_iou: 0.8698021173477173, precision: 0.8780218614269266, recall: 0.820690338788968, dice: 0.8483886308357546, acc: 0.8780218622173661\n",
      "inference: 56.80137634277344, batch_iou: 0.9166103601455688, precision: 0.9123033937548445, recall: 0.9174418131868742, dice: 0.9148653884166594, acc: 0.9123033941485917\n",
      "inference: 56.92502212524414, batch_iou: 0.8500760197639465, precision: 0.8117171866759939, recall: 0.9007750838876993, dice: 0.8539304141279943, acc: 0.811717186977467\n",
      "inference: 56.825984954833984, batch_iou: 0.8845674991607666, precision: 0.8998765552440612, recall: 0.918233931145602, dice: 0.9089625663964293, acc: 0.8998765556103386\n",
      "inference: 56.793888092041016, batch_iou: 0.8866233825683594, precision: 0.8648763422010671, recall: 0.8907399311594354, dice: 0.8776176265453121, acc: 0.86487634280591\n",
      "inference: 56.79724884033203, batch_iou: 0.8978995680809021, precision: 0.903387875342507, recall: 0.8892559989101787, dice: 0.8962662344699495, acc: 0.9033878759302395\n",
      "inference: 56.7762565612793, batch_iou: 0.8757016658782959, precision: 0.8789661375288943, recall: 0.9326015933033582, dice: 0.9049898674501883, acc: 0.8789661378797838\n",
      "inference: 56.735809326171875, batch_iou: 0.8854628801345825, precision: 0.8703672500600438, recall: 0.843292160162435, dice: 0.8566158176062797, acc: 0.8703672507015575\n",
      "inference: 56.756065368652344, batch_iou: 0.8479902148246765, precision: 0.8036307972487152, recall: 0.8164300468860636, dice: 0.8099798619150521, acc: 0.8036307979088438\n",
      "inference: 56.7724494934082, batch_iou: 0.9227694272994995, precision: 0.942661224602068, recall: 0.9383587605614118, dice: 0.9405050720473014, acc: 0.9426612250276786\n",
      "inference: 56.73817443847656, batch_iou: 0.8884381055831909, precision: 0.8437453422733238, recall: 0.8999956877651877, dice: 0.8709632410936728, acc: 0.8437453428948454\n",
      "inference: 56.8966064453125, batch_iou: 0.8949058055877686, precision: 0.9030868428984649, recall: 0.8890655516482512, dice: 0.8960213479439003, acc: 0.9030868434158823\n",
      "inference: 56.87846374511719, batch_iou: 0.9243721961975098, precision: 0.938066223246608, recall: 0.9250453984387432, dice: 0.9315103111719701, acc: 0.9380662236306183\n",
      "inference: 56.77040100097656, batch_iou: 0.8859092593193054, precision: 0.9106208576074073, recall: 0.9065666663053344, dice: 0.9085892394323608, acc: 0.9106208580541002\n",
      "inference: 56.81353759765625, batch_iou: 0.8837621212005615, precision: 0.8260583627519117, recall: 0.9098483021556698, dice: 0.8659311171494016, acc: 0.8260583631048956\n",
      "inference: 56.887168884277344, batch_iou: 0.9221113324165344, precision: 0.9102756319882713, recall: 0.9428452750819553, dice: 0.9262742386046317, acc: 0.9102756325902104\n",
      "inference: 56.835777282714844, batch_iou: 0.9150575399398804, precision: 0.9162826918541785, recall: 0.9161759037077225, dice: 0.9162292946442178, acc: 0.9162826924042983\n",
      "inference: 56.78799819946289, batch_iou: 0.8735504150390625, precision: 0.8611954618953834, recall: 0.8434810125130672, dice: 0.8522461957300319, acc: 0.861195462429317\n",
      "inference: 56.846431732177734, batch_iou: 0.8664289712905884, precision: 0.874883007896947, recall: 0.8828865784337425, dice: 0.8788665719958155, acc: 0.874883008281081\n",
      "inference: 56.84975814819336, batch_iou: 0.8459389805793762, precision: 0.8103954477448451, recall: 0.8242761716502969, dice: 0.8172768759355105, acc: 0.8103954482839536\n",
      "inference: 56.90339279174805, batch_iou: 0.9208001494407654, precision: 0.9251693943444886, recall: 0.9325250123918971, dice: 0.9288326409163772, acc: 0.9251693947188857\n",
      "inference: 56.842430114746094, batch_iou: 0.8934577703475952, precision: 0.9111911823376382, recall: 0.8688410332258383, dice: 0.8895123148636523, acc: 0.9111911829002225\n",
      "inference: 56.853759765625, batch_iou: 0.8664954900741577, precision: 0.8245149361970328, recall: 0.8639730799285357, dice: 0.8437829608718124, acc: 0.824514936829048\n",
      "inference: 56.835201263427734, batch_iou: 0.8750805854797363, precision: 0.8374022728785093, recall: 0.8772400829713406, dice: 0.8568583842380331, acc: 0.8374022735525174\n",
      "inference: 56.857505798339844, batch_iou: 0.9288417100906372, precision: 0.9403880865570727, recall: 0.9431111548188832, dice: 0.9417476522408811, acc: 0.9403880868691934\n",
      "inference: 56.78220748901367, batch_iou: 0.851606011390686, precision: 0.7547406211197897, recall: 0.8560056934584606, dice: 0.8021899697534233, acc: 0.7547406218056137\n",
      "inference: 56.93062210083008, batch_iou: 0.9218367338180542, precision: 0.9177553445257329, recall: 0.9334124212188876, dice: 0.9255176695032281, acc: 0.9177553449114396\n",
      "inference: 56.74028778076172, batch_iou: 0.9095290899276733, precision: 0.9206959653511106, recall: 0.9247141066170479, dice: 0.922700661478266, acc: 0.9206959657049514\n",
      "inference: 56.92841720581055, batch_iou: 0.8842397332191467, precision: 0.8859798367145021, recall: 0.8531283569651811, dice: 0.8692438171637579, acc: 0.8859798372398812\n",
      "inference: 56.80192184448242, batch_iou: 0.9147889614105225, precision: 0.8947713630750305, recall: 0.9557713021093065, dice: 0.924265953823474, acc: 0.8947713633846459\n",
      "inference: 56.87433624267578, batch_iou: 0.9095028638839722, precision: 0.924906860845091, recall: 0.940764348462872, dice: 0.9327682133712218, acc: 0.9249068611262825\n",
      "inference: 56.832096099853516, batch_iou: 0.8951706886291504, precision: 0.9157649400367388, recall: 0.9040644258338407, dice: 0.9098770689408628, acc: 0.9157649404337161\n",
      "inference: 56.768287658691406, batch_iou: 0.8287353515625, precision: 0.8455615535560032, recall: 0.7147457119895366, dice: 0.7746698460024857, acc: 0.8455615542805576\n",
      "inference: 56.78636932373047, batch_iou: 0.8887212872505188, precision: 0.8632744585918898, recall: 0.8809689694947075, dice: 0.8720319628527126, acc: 0.8632744590995627\n",
      "inference: 56.81071853637695, batch_iou: 0.8592876195907593, precision: 0.9116281145640069, recall: 0.8819956758896486, dice: 0.8965671166099674, acc: 0.9116281149607296\n",
      "inference: 56.823936462402344, batch_iou: 0.8264076709747314, precision: 0.6983150591118239, recall: 0.8753642665204866, dice: 0.7768800663515405, acc: 0.698315059562277\n",
      "inference: 56.758846282958984, batch_iou: 0.7913623452186584, precision: 0.5862518150164138, recall: 0.8420626839569982, dice: 0.6912494091401318, acc: 0.5862518154067675\n",
      "inference: 56.79804611206055, batch_iou: 0.8479745388031006, precision: 0.8503713069973183, recall: 0.7798655823190257, dice: 0.8135938020011428, acc: 0.8503713075606887\n",
      "inference: 56.78927993774414, batch_iou: 0.8680644035339355, precision: 0.8068423293142933, recall: 0.8936316879346216, dice: 0.8480222164859376, acc: 0.8068423299101398\n",
      "inference: 56.95868682861328, batch_iou: 0.8736013770103455, precision: 0.8339785467121443, recall: 0.8722197633802342, dice: 0.852670602630548, acc: 0.8339785472392904\n",
      "inference: 56.92559814453125, batch_iou: 0.9381079077720642, precision: 0.946101620285258, recall: 0.9497945811942679, dice: 0.9479445040284916, acc: 0.9461016204976495\n",
      "inference: 56.757598876953125, batch_iou: 0.9016040563583374, precision: 0.9122401482992124, recall: 0.9220205699924562, dice: 0.9171042841433736, acc: 0.9122401486393019\n",
      "inference: 56.835777282714844, batch_iou: 0.8479318618774414, precision: 0.6779247754393694, recall: 0.8627074769303981, dice: 0.759234751310783, acc: 0.6779247759174457\n",
      "inference: 56.84979248046875, batch_iou: 0.8934042453765869, precision: 0.8562479318390537, recall: 0.9066626215351669, dice: 0.8807344116822813, acc: 0.8562479322824338\n",
      "inference: 56.864479064941406, batch_iou: 0.8775070309638977, precision: 0.8794516284295738, recall: 0.929672456158231, dice: 0.9038649835318399, acc: 0.8794516286848731\n",
      "inference: 56.85551834106445, batch_iou: 0.8191719055175781, precision: 0.5835110383617444, recall: 0.8803224408633782, dice: 0.7018255406443442, acc: 0.583511038700099\n",
      "inference: 56.73164749145508, batch_iou: 0.8367985486984253, precision: 0.8113131703687217, recall: 0.8097771089074502, dice: 0.8105444118053811, acc: 0.8113131711096991\n",
      "inference: 56.78799819946289, batch_iou: 0.8048056364059448, precision: 0.6937446342383388, recall: 0.8149993521953091, dice: 0.74949949430168, acc: 0.6937446346587346\n",
      "inference: 56.7836799621582, batch_iou: 0.9232074618339539, precision: 0.9140308203923938, recall: 0.9279635552745525, dice: 0.9209444946337286, acc: 0.9140308207277662\n",
      "inference: 56.832096099853516, batch_iou: 0.8690453767776489, precision: 0.8723872886597829, recall: 0.8391684218597849, dice: 0.8554554896696424, acc: 0.8723872894577075\n",
      "inference: 56.76291275024414, batch_iou: 0.815811038017273, precision: 0.8187337576700803, recall: 0.7471461743302593, dice: 0.7813035754032917, acc: 0.8187337584334432\n",
      "inference: 56.83184051513672, batch_iou: 0.8725634813308716, precision: 0.9211581106352856, recall: 0.9028843726132391, dice: 0.9119297060423471, acc: 0.9211581110927881\n",
      "inference: 56.73990249633789, batch_iou: 0.8582034707069397, precision: 0.8519978540260493, recall: 0.9108258431211557, dice: 0.8804302607910311, acc: 0.8519978544442742\n",
      "inference: 56.835487365722656, batch_iou: 0.8930458426475525, precision: 0.9196492026422819, recall: 0.9466547742535277, dice: 0.9329566020181137, acc: 0.9196492029599853\n",
      "inference: 56.80137634277344, batch_iou: 0.8418670892715454, precision: 0.7799146701125821, recall: 0.871537622612956, dice: 0.8231845150465295, acc: 0.7799146708819833\n",
      "inference: 56.78473663330078, batch_iou: 0.8288342356681824, precision: 0.7658639945271234, recall: 0.8228056801293075, dice: 0.7933143747878804, acc: 0.7658639950330587\n",
      "inference: 56.717472076416016, batch_iou: 0.9112106561660767, precision: 0.8913034821717406, recall: 0.9044124764242838, dice: 0.8978101304480299, acc: 0.8913034829274051\n",
      "inference: 56.80976104736328, batch_iou: 0.7835826277732849, precision: 0.668703372327971, recall: 0.6868033465816458, dice: 0.6776325155563225, acc: 0.6687033728787072\n",
      "inference: 56.778560638427734, batch_iou: 0.9164618849754333, precision: 0.9089356604055335, recall: 0.94192910224985, dice: 0.9251383114203364, acc: 0.9089356607524594\n",
      "inference: 56.7786865234375, batch_iou: 0.8764607906341553, precision: 0.8557994275142601, recall: 0.9323199055076343, dice: 0.8924223642456427, acc: 0.8557994280127748\n",
      "inference: 56.867454528808594, batch_iou: 0.8918724656105042, precision: 0.9004304979033516, recall: 0.9297558476326815, dice: 0.9148582305173415, acc: 0.9004304982537052\n",
      "inference: 56.82476806640625, batch_iou: 0.9106041193008423, precision: 0.9295554118740585, recall: 0.8801338718608566, dice: 0.9041698053843292, acc: 0.9295554123496446\n",
      "inference: 56.778079986572266, batch_iou: 0.9302284121513367, precision: 0.9369521275987474, recall: 0.9390565930766344, dice: 0.9380031799565621, acc: 0.9369521278854943\n",
      "inference: 56.70441436767578, batch_iou: 0.7886170744895935, precision: 0.70331316596122, recall: 0.7423054953416223, dice: 0.7222834651030436, acc: 0.7033131665576301\n",
      "inference: 56.84940719604492, batch_iou: 0.8141337633132935, precision: 0.771278865002122, recall: 0.7577748295056771, dice: 0.7644672158228569, acc: 0.7712788657479875\n",
      "inference: 56.812416076660156, batch_iou: 0.9089797735214233, precision: 0.9231121984858347, recall: 0.8885980836552595, dice: 0.9055263842549238, acc: 0.9231121989467879\n",
      "inference: 56.786495208740234, batch_iou: 0.8356475830078125, precision: 0.6438904064322866, recall: 0.885839030836937, dice: 0.7457308980898885, acc: 0.6438904067439916\n",
      "inference: 56.86307144165039, batch_iou: 0.8758641481399536, precision: 0.8849590549575488, recall: 0.8535014427973354, dice: 0.8689456345523682, acc: 0.8849590555890968\n",
      "inference: 56.787841796875, batch_iou: 0.9332762956619263, precision: 0.947512716238116, recall: 0.9432756865801133, dice: 0.9453894540626752, acc: 0.9475127165587279\n",
      "inference: 56.72796630859375, batch_iou: 0.8769845366477966, precision: 0.84548811149019, recall: 0.9210752176630629, dice: 0.8816645669539888, acc: 0.8454881118781837\n",
      "inference: 56.78403091430664, batch_iou: 0.8417684435844421, precision: 0.8734501611422502, recall: 0.7964143253558204, dice: 0.8331552965910055, acc: 0.8734501619375491\n",
      "inference: 56.86812973022461, batch_iou: 0.8477968573570251, precision: 0.8175844588556546, recall: 0.8691248718622236, dice: 0.8425672106849657, acc: 0.8175844592121652\n",
      "inference: 56.77827072143555, batch_iou: 0.8921290636062622, precision: 0.9104380737969204, recall: 0.896443011271314, dice: 0.9033863436497427, acc: 0.9104380743378737\n",
      "inference: 56.760353088378906, batch_iou: 0.8313363790512085, precision: 0.7974269939499684, recall: 0.7931225841066779, dice: 0.7952689645480855, acc: 0.7974269946990619\n",
      "inference: 56.919776916503906, batch_iou: 0.8929712176322937, precision: 0.856735707134129, recall: 0.8814816373156347, dice: 0.8689325258891073, acc: 0.8567357077698631\n",
      "inference: 56.7657585144043, batch_iou: 0.9353737235069275, precision: 0.9211031817011643, recall: 0.9472992383352108, dice: 0.9340175682560238, acc: 0.9211031820144536\n",
      "inference: 56.84291076660156, batch_iou: 0.8129628300666809, precision: 0.7472532348218783, recall: 0.8165373287822402, dice: 0.7803604579937522, acc: 0.747253235447998\n",
      "inference: 56.76063919067383, batch_iou: 0.929463267326355, precision: 0.9185715624286517, recall: 0.9342456318571233, dice: 0.9263422990452079, acc: 0.9185715628171579\n",
      "inference: 56.76278305053711, batch_iou: 0.885498046875, precision: 0.8964592892557414, recall: 0.9130734041746191, dice: 0.9046900759458746, acc: 0.8964592897022221\n",
      "inference: 56.73030471801758, batch_iou: 0.9146682024002075, precision: 0.9018331145018681, recall: 0.9304589527183816, dice: 0.9159224233312964, acc: 0.9018331148851391\n",
      "inference: 56.841217041015625, batch_iou: 0.8961024284362793, precision: 0.8691340938574007, recall: 0.8847129538291958, dice: 0.8768543327968985, acc: 0.8691340944432989\n",
      "inference: 56.779903411865234, batch_iou: 0.8834008574485779, precision: 0.9345132042810154, recall: 0.9121449307545295, dice: 0.9231935958459951, acc: 0.934513204740898\n",
      "inference: 56.77657699584961, batch_iou: 0.8885068893432617, precision: 0.883169641668381, recall: 0.8975008649028287, dice: 0.8902775828592144, acc: 0.8831696420959664\n",
      "inference: 56.79865646362305, batch_iou: 0.8586373925209045, precision: 0.8248148684069018, recall: 0.8370926160325209, dice: 0.8309083897352529, acc: 0.8248148689312353\n",
      "inference: 56.79302215576172, batch_iou: 0.8146159648895264, precision: 0.7697053697017826, recall: 0.7519543941854508, dice: 0.7607263445305869, acc: 0.7697053703416553\n",
      "inference: 56.80934524536133, batch_iou: 0.8498205542564392, precision: 0.853043615652733, recall: 0.8175678255759699, dice: 0.8349290525736376, acc: 0.8530436162695016\n",
      "inference: 16.757919311523438, batch_iou: 0.21306104958057404, precision: 0.8710361867780418, recall: 0.770929660849417, dice: 0.8179313019505, acc: 0.8710361905792846\n",
      "MEAN inference: 56.60277509689331, batch_iou: 0.8663958311080933, precision: 0.8459188223129863, recall: 0.8736673369059867, dice: 0.8578825849645517, acc: 0.8459188228339278\n"
     ]
    }
   ],
   "source": [
    "model.train(False)\n",
    "model.to(device)\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "jaccard = JaccardIndex(num_classes=2)\n",
    "mIoU = 0\n",
    "Precision = 0\n",
    "Recall = 0 \n",
    "Dice = 0\n",
    "Pixel_accuracy = 0 \n",
    "time = 0\n",
    "metric_calculator = metrics.SegmentationMetrics(average=True, ignore_background=True, activation=\"softmax\")\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        tinputs, tlabels = data\n",
    "        tinputs = torch.tensor(tinputs).permute(0,3,1,2).to(device=device, dtype=torch.float)\n",
    "        tlabels = torch.tensor(tlabels).to(device=device, dtype=torch.long)\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        starter.record()\n",
    "        toutputs = model(tinputs)\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        batch_iou = 0\n",
    "        for i in range(len(toutputs)):\n",
    "            pred = F.softmax(toutputs, dim=1)[i].permute(1,2,0)[:,:, 1]\n",
    "            batch_iou += jaccard(pred.cpu(), tlabels[i].cpu().int())\n",
    "        mIoU += batch_iou/batch_size\n",
    "        time += curr_time\n",
    "        pixel_accuracy, dice, precision, recall = metric_calculator(tlabels.int(), toutputs)\n",
    "        Pixel_accuracy += pixel_accuracy\n",
    "        Dice += dice\n",
    "        Recall += recall\n",
    "        Precision += precision\n",
    "        print(\"inference: {0}, batch_iou: {1}, precision: {2}, recall: {3}, dice: {4}, acc: {5}\".format(curr_time, batch_iou/batch_size, precision, recall, dice, pixel_accuracy))\n",
    "mIoU /= len(test_loader)\n",
    "mTime = time / len(test_loader)\n",
    "Precision /= len(test_loader)\n",
    "Dice /= len(test_loader)\n",
    "Recall /= len(test_loader)\n",
    "Pixel_accuracy /= len(test_loader)\n",
    "# print(\"mean inference: {0}, mean iou: {1}\".format( mTime, mIoU))\n",
    "print(\"MEAN inference: {0}, batch_iou: {1}, precision: {2}, recall: {3}, dice: {4}, acc: {5}\".format(mTime, mIoU, Precision, Recall, Dice, Pixel_accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
