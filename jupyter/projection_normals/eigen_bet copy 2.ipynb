{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laserscan import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flaten_3x3(m):\n",
    "     m = np.pad(m, [(1, 1), (1, 1)], mode='constant')\n",
    "     arr = np.array([np.matrix([m[i][j:j+3], m[i+1][j:j+3], m[i+2][j:j+3]]).flatten(\"C\") for j in range(len(m[0])-2) for i in range(len(m)-2)])\n",
    "     arr_s = np.squeeze(arr).reshape((len(m)-2, len(m[0])-2, 9))\n",
    "     return np.transpose(arr_s, axes=(1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700\n",
      "300\n",
      "torch.Size([1, 1024, 64, 3, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f319124b9d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x5760 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from laserscan import *\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "device = \"cuda:0\"\n",
    "# device=\"cpu\"\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class CustomKitti(Dataset):\n",
    "    def __init__(self, dir, mode=\"train\"):\n",
    "        self.ls = SemLaserScan(project=True, nclasses=100)\n",
    "        self.mode = mode\n",
    "        self.len = 4541\n",
    "        self.train_len = 3700\n",
    "        self.val_len = 300\n",
    "        self.test_len = self.len - self.train_len - self.val_len\n",
    "        self.label_folder = dir + \"plane_labels/\"\n",
    "        self.file_folder = dir + \"velodyne/\"\n",
    "    def __len__(self):\n",
    "        if self.mode == \"train\":\n",
    "            return self.train_len\n",
    "        if self.mode == \"val\":\n",
    "            return self.val_len\n",
    "        return self.test_len\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            idx += self.train_len + self.val_len\n",
    "        if self.mode == \"val\":\n",
    "            idx += self.train_len\n",
    "        self.ls.open_scan(self.file_folder + '{0:06d}.bin'.format(idx))\n",
    "        self.ls.open_label(self.label_folder + 'label-{0:06d}.npy'.format(idx))\n",
    "        self.ls.proj_sem_label[self.ls.proj_sem_label != 0] = 1\n",
    "        # print(\"points shape: \", self.ls.proj_xyz.shape)\n",
    "        # print(\"shape x : \", flaten_3x3(self.ls.proj_xyz[:,:, 0]).shape)\n",
    "        return np.stack((flaten_3x3(self.ls.proj_xyz[:,:, 0]),flaten_3x3(self.ls.proj_xyz[:,:, 1]), flaten_3x3(self.ls.proj_xyz[:,:, 2])), axis=2).squeeze(), self.ls.proj_sem_label\n",
    "\n",
    "training_data = CustomKitti(\"/home/polosatik/mnt/kitty/dataset/sequences/00/\") \n",
    "validation_data = CustomKitti(\"/home/polosatik/mnt/kitty/dataset/sequences/00/\", mode=\"val\") \n",
    "test_data = CustomKitti(\"/home/polosatik/mnt/kitty/dataset/sequences/00/\", mode=\"test\") \n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "training_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "print(len(training_loader))\n",
    "print(len(validation_loader))\n",
    "\n",
    "vinputs, vlabels = next(iter(test_loader))\n",
    "print(vinputs.shape)\n",
    "plt.figure(figsize=(20,80))\n",
    "plt.imshow(vlabels[0], cmap=\"gray\")\n",
    "# validation_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):\n",
    "    # Average of Dice coefficient for all batches, or for a single mask\n",
    "    assert input.size() == target.size()\n",
    "    if input.dim() == 2 and reduce_batch_first:\n",
    "        raise ValueError(f'Dice: asked to reduce batch but got tensor without batch dimension (shape {input.shape})')\n",
    "\n",
    "    if input.dim() == 2 or reduce_batch_first:\n",
    "        inter = torch.dot(input.reshape(-1), target.reshape(-1))\n",
    "        sets_sum = torch.sum(input) + torch.sum(target)\n",
    "        if sets_sum.item() == 0:\n",
    "            sets_sum = 2 * inter\n",
    "\n",
    "        return (2 * inter + epsilon) / (sets_sum + epsilon)\n",
    "    else:\n",
    "        # compute and average metric for each batch element\n",
    "        dice = 0\n",
    "        for i in range(input.shape[0]):\n",
    "            dice += dice_coeff(input[i, ...], target[i, ...])\n",
    "        return dice / input.shape[0]\n",
    "\n",
    "\n",
    "def multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):\n",
    "    # Average of Dice coefficient for all classes\n",
    "    assert input.size() == target.size()\n",
    "    dice = 0\n",
    "    for channel in range(input.shape[1]):\n",
    "        dice += dice_coeff(input[:, channel, ...], target[:, channel, ...], reduce_batch_first, epsilon)\n",
    "\n",
    "    return dice / input.shape[1]\n",
    "\n",
    "\n",
    "def dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):\n",
    "    # Dice loss (objective to minimize) between 0 and 1\n",
    "    assert input.size() == target.size()\n",
    "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
    "    return 1 - fn(input, target, reduce_batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch-vectorized\n",
    "import torch\n",
    "from torchvectorized.utils import sym\n",
    "from torchvectorized.vlinalg import vSymEig\n",
    "from torchvectorized.nn import EigVals\n",
    "\n",
    "# Random batch of volumetric 3x3 symmetric matrices of size 16x9x32x32x32\n",
    "input = sym(torch.rand(4, 9, 3, 1024, 64))\n",
    "\n",
    "# Output eig_vals with size: 16x3x32x32x32 and eig_vecs with size 16,3,3,32,32,32\n",
    "eig_vals, eig_vecs = vSymEig(input, eigenvectors=True)\n",
    "# eig_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4, 2, 5]) torch.Size([1, 3, 3, 4, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvectorized.utils import sym\n",
    "from torchvectorized.vlinalg import vSymEig\n",
    "\n",
    "b, c, d, h, w = 1, 9, 4, 2, 5\n",
    "inputs = sym(torch.rand(b, c, d, h, w))\n",
    "v, u = vSymEig(inputs, eigenvectors=True)\n",
    "print(v.shape, u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvectorized.vlinalg import vSymEig\n",
    "import time\n",
    "\n",
    "\n",
    "#do some stuff\n",
    "\n",
    "\n",
    "\n",
    "def _grad_sym(X):\n",
    "    return 0.5 * (X + X.transpose(1, 2))\n",
    "\n",
    "def floor_div(input, K):\n",
    "    rem = torch.remainder(input, K)\n",
    "    out = (input - rem) / K\n",
    "    return out\n",
    "\n",
    "def normal(m):\n",
    "    # t = time.process_time()\n",
    "    p = torch.sum(torch.sum(m, axis=1), axis=0)\n",
    "    arr = (m.permute(1, 2, 0).flatten(0, 1) - floor_div(p, 9)).reshape(9,3,1)\n",
    "    cov_m = torch.zeros(3, 3).to(device)\n",
    "    for vec in arr:\n",
    "        cov_m += vec@vec.T\n",
    "    cov_m/9\n",
    "    L, V = torch.linalg.eig(cov_m/9)\n",
    "    L, V = torch.real(L),torch.real(V)\n",
    "    # elapsed_time = time.process_time() - t\n",
    "    # print(\"norm \",elapsed_time)\n",
    "    return V[torch.argmin(L)]\n",
    "\n",
    "class EigValsFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X):\n",
    "        t = time.process_time()\n",
    "\n",
    "        # torch.Size([1, 1024, 64, 3, 9])\n",
    "        # print(\"X \", X.shape)\n",
    "        X = X.reshape(1024*64, 3, 3, 3)\n",
    "\n",
    "        tList = [normal(a) for a in torch.unbind(X, dim=0) ]\n",
    "        res = torch.stack(tList, dim=0).reshape(1, 1024, 64, 3)\n",
    "        # print(\"res \", res.shape)\n",
    "        \n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        grad_input = grad_outputs.clone()\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class EigVals(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Differentiable neural network layer (:class:`torch.nn.Module`) that performs eigendecomposition on\n",
    "    every voxel in a volume of flattened 3x3 symmetric matrices of shape **Bx9xDxHxW** and return the eigenvalues.\n",
    "\n",
    "    See **Ionescu et al., Matrix backpropagation for deep networks with structured layers, CVPR 2015** for details on the\n",
    "    gradients computation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "                                                                    ([1, 9, 3, 64, 1024])\n",
    "        Takes a volume of flattened 3x3 symmetric matrices of shape **Bx9xDxHxW** and return a volume of their eigenvalues\n",
    "\n",
    "        :param x: A volume of flattened 3x3 symmetric matrices of shape **Bx9xDxHxW**\n",
    "        :type x: torch.Tensor\n",
    "        :return: A tensor with shape **(B*D*H*W)x3** where every voxel's channels are the eigenvalues of the inpur matrix\n",
    "                                    ([1, 3, 64, 3, 9])\n",
    "            at the same spatial location.\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        t = time.process_time()\n",
    "\n",
    "        X = x.reshape(1024*64, 3, 3, 3)\n",
    "\n",
    "        tList = [normal(a) for a in torch.unbind(X, dim=0) ]\n",
    "        res = torch.stack(tList, dim=0).reshape(1, 1024, 64, 3)\n",
    "        # print(\"res \", res.shape)\n",
    "        elapsed_time = time.process_time() - t\n",
    "        print(\"forw \",elapsed_time)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_classes = 2\n",
    "from unet import UNet\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# import torchvectorized\n",
    "def get_model_and_optimizer(device, num_encoding_blocks=5, out_channels_first_layer=8, patience=3):\n",
    "    #Better to train with num_encoding_blocks >=3, out_channels_first_layer>=4 '''\n",
    "    #repoducibility\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "      \n",
    "    # unet = UNet(\n",
    "    #       in_channels=5,\n",
    "    #       out_classes=max_classes,\n",
    "    #       dimensions=2,\n",
    "    #       num_encoding_blocks=num_encoding_blocks,\n",
    "    #       normalization='batch',\n",
    "    #       upsampling_type='linear',\n",
    "    #       padding=True,\n",
    "    #       activation='ReLU',\n",
    "    #   ).to(device)\n",
    "    # print( \"unet par:\", unet.parameters())\n",
    "    # model = unet\n",
    "    EV = EigVals()\n",
    "    # print(\"par: \", list(EV.parameters()))\n",
    "    model = nn.Sequential(nn.Linear(in_features=9, out_features=9, bias=False), EV, nn.Linear(3, 1, bias=False), nn.Softmax())\n",
    "    model.to(device=device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=patience, threshold=0.01)\n",
    "    \n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "def train_one_epoch(epoch_index, tb_writer, optimizer, model):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        t = time.process_time()\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = torch.tensor(inputs).to(device=device, dtype=torch.float)\n",
    "        # print(inputs.shape)\n",
    "        labels = torch.tensor(labels).to(device=device, dtype=torch.float)\n",
    "        # print(torch.max(labels))\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs).squeeze(dim=3)\n",
    "\n",
    "        # print(outputs.shape, labels.shape)\n",
    "        # outputs = outputs.permute(0, 2, 1)\n",
    "        # print(outputs.shape, labels.shape)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = torch.nn.functional.mse_loss(outputs.permute(0, 2, 1), labels)\n",
    "\n",
    "        # loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        elapsed_time = time.process_time() - t\n",
    "        print(\"step \",elapsed_time)\n",
    "        if i % 100== 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            # tb_writer.add_image(\"label\", visualize_seg(labels.numpy(), mc).squeeze()[0], 0)\n",
    "            # tb_writer.add_image(\"label\", visualize_seg(outputs.cpu().numpy(), mc).squeeze()[0], 0)\n",
    "\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# run_name = \"SEMANTIC_log softmax_nll_\"\n",
    "run_name = \"SEMANTIC_cross_entropy_dice_softmax_5 s\"\n",
    "writer = SummaryWriter('runs/{}_{}'.format(run_name, timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 4\n",
    "\n",
    "best_vloss = 1_000_00000.\n",
    "model, optimizer, scheduler = get_model_and_optimizer(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36063/2770792307.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs).to(device=device, dtype=torch.float)\n",
      "/tmp/ipykernel_36063/2770792307.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device=device, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forw  33.080017678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polosatik/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  307.59614809500005\n",
      "forw  34.031080488999976\n"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "if train :\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss = train_one_epoch(epoch_number, writer, optimizer, model)\n",
    "\n",
    "        with torch.no_grad():\n",
    "        # We don't need gradients on to do reporting\n",
    "            model.train(False)\n",
    "\n",
    "            running_vloss = 0.0\n",
    "            for i, vdata in enumerate(validation_loader):\n",
    "                vinputs, vlabels = vdata\n",
    "                vinputs = torch.tensor(vinputs).to(device=device, dtype=torch.float)\n",
    "                vlabels = torch.tensor(vlabels).to(device=device, dtype=torch.float)\n",
    "\n",
    "                voutputs = model(vinputs).squeeze(dim=3)\n",
    "                vloss = torch.nn.functional.mse_loss(voutputs.permute(0, 2, 1), vlabels)\n",
    "                running_vloss += vloss\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "        # Log the running loss averaged per batch\n",
    "        # for both training and validation\n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                        { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                        epoch_number + 1)\n",
    "        writer.flush()\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = 'chpt/model_{}_{}_{}'.format(run_name, timestamp, epoch_number)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196608\n",
      "192.0\n",
      "1728\n"
     ]
    }
   ],
   "source": [
    "print(64*1024*3)\n",
    "print(1728/9)\n",
    "print(64*3*9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9061)\n",
      "torch.Size([64, 1024]) torch.Size([2, 64, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9218953322843892, 0.9256186629880894, 0.92189533192352, 0.9293721914617133)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchmetrics import JaccardIndex\n",
    "import metrics\n",
    "\n",
    "jaccard = JaccardIndex(num_classes=2)\n",
    "pred = F.softmax(voutputs, dim=1)[0].permute(1,2,0).detach().squeeze()[:,:, 1]\n",
    "vlabels.shape\n",
    "print(jaccard(pred.cpu(), vlabels[0].int().cpu()))\n",
    "metric_calculator = metrics.SegmentationMetrics(average=True, ignore_background=True, activation=\"softmax\")\n",
    "print(vlabels[0].int().shape, voutputs[0].shape)\n",
    "metric_calculator(vlabels.int(), voutputs)\n",
    "# pred\n",
    "# F.softmax(voutputs, dim=1)[0].cpu().permute(1,2,0).detach().numpy().squeeze()[:,:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14367/1610888461.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tinputs = torch.tensor(tinputs).permute(0,3,1,2).to(device=device, dtype=torch.float)\n",
      "/tmp/ipykernel_14367/1610888461.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tlabels = torch.tensor(tlabels).to(device=device, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference: 64.68943786621094, batch_iou: 0.8447953462600708, precision: 0.7793742083436951, recall: 0.8479480097156241, dice: 0.8122162917723272, acc: 0.7793742088654328\n",
      "inference: 59.759742736816406, batch_iou: 0.8830991983413696, precision: 0.847964815312653, recall: 0.8930762186791857, dice: 0.8699360853724435, acc: 0.8479648159053155\n",
      "inference: 58.6673583984375, batch_iou: 0.9214291572570801, precision: 0.9258089691020912, recall: 0.9449404892353417, dice: 0.935276903374103, acc: 0.9258089694196798\n",
      "inference: 56.51443099975586, batch_iou: 0.8905607461929321, precision: 0.904352996881189, recall: 0.9082917699426406, dice: 0.9063181040228906, acc: 0.9043529973648238\n",
      "inference: 56.59910583496094, batch_iou: 0.905887246131897, precision: 0.8841247129011479, recall: 0.927893137910955, dice: 0.905480322684691, acc: 0.8841247134371755\n",
      "inference: 56.567134857177734, batch_iou: 0.7872521281242371, precision: 0.553210848388902, recall: 0.8677899469837331, dice: 0.6756798649185617, acc: 0.5532108486285355\n",
      "inference: 56.63711929321289, batch_iou: 0.8604093790054321, precision: 0.8313475645053379, recall: 0.8343338625372185, dice: 0.8328380364862088, acc: 0.8313475650991954\n",
      "inference: 56.536705017089844, batch_iou: 0.8427658081054688, precision: 0.8640434757358865, recall: 0.7669203227739405, dice: 0.8125900793866195, acc: 0.8640434763151927\n",
      "inference: 56.66041564941406, batch_iou: 0.8879827857017517, precision: 0.8684469939276822, recall: 0.8862654198659136, dice: 0.8772657372403554, acc: 0.8684469945486138\n",
      "inference: 56.521793365478516, batch_iou: 0.8669235706329346, precision: 0.8106472924073891, recall: 0.8532580502141522, dice: 0.8314070642868573, acc: 0.8106472929800872\n",
      "inference: 56.8026237487793, batch_iou: 0.8340128660202026, precision: 0.6416510926425486, recall: 0.8924464414805342, dice: 0.746548666559705, acc: 0.6416510930106443\n",
      "inference: 56.73984146118164, batch_iou: 0.8115527033805847, precision: 0.8080828836193336, recall: 0.7441803580815185, dice: 0.774816272750449, acc: 0.8080828842007879\n",
      "inference: 56.79024124145508, batch_iou: 0.8755879998207092, precision: 0.8280499989948485, recall: 0.8970635655127894, dice: 0.8611763303664286, acc: 0.8280499996220299\n",
      "inference: 56.785377502441406, batch_iou: 0.800683856010437, precision: 0.6922248292225986, recall: 0.8588278220786533, dice: 0.7665786740838897, acc: 0.6922248294684852\n",
      "inference: 56.94617462158203, batch_iou: 0.8062745928764343, precision: 0.7423341685520108, recall: 0.7782121482021861, dice: 0.7598498796586219, acc: 0.742334169284665\n",
      "inference: 56.781822204589844, batch_iou: 0.8356263637542725, precision: 0.77407026474743, recall: 0.8277347144292249, dice: 0.8000035432296748, acc: 0.7740702655585965\n",
      "inference: 56.90969467163086, batch_iou: 0.8641153573989868, precision: 0.8699163913930877, recall: 0.892570821035518, dice: 0.8810980099042337, acc: 0.8699163917493367\n",
      "inference: 56.9317741394043, batch_iou: 0.8429806232452393, precision: 0.8681465605317767, recall: 0.8873852867878388, dice: 0.8776605058405188, acc: 0.868146560892103\n",
      "inference: 56.776798248291016, batch_iou: 0.8527064323425293, precision: 0.8756158548549194, recall: 0.8782593928132088, dice: 0.8769356315620024, acc: 0.8756158552615051\n",
      "inference: 56.830623626708984, batch_iou: 0.8586559891700745, precision: 0.8968171828886802, recall: 0.9029211987491926, dice: 0.8998588395671279, acc: 0.8968171832890371\n",
      "inference: 56.949951171875, batch_iou: 0.8930860161781311, precision: 0.8963273135312354, recall: 0.8743749199900425, dice: 0.8852150386361178, acc: 0.8963273139592756\n",
      "inference: 56.77833557128906, batch_iou: 0.8213704824447632, precision: 0.8486718470091147, recall: 0.6978000155556471, dice: 0.7658764990195623, acc: 0.8486718478186549\n",
      "inference: 57.00041580200195, batch_iou: 0.8873710632324219, precision: 0.9149921167935086, recall: 0.8629111192391715, dice: 0.888188800794578, acc: 0.9149921173050912\n",
      "inference: 56.855712890625, batch_iou: 0.8623924851417542, precision: 0.8872321199201628, recall: 0.811636120977896, dice: 0.8477521903449299, acc: 0.8872321207044025\n",
      "inference: 56.74854278564453, batch_iou: 0.8760720491409302, precision: 0.8793128889611087, recall: 0.8308228551248465, dice: 0.8543804168054059, acc: 0.8793128895680564\n",
      "inference: 56.762718200683594, batch_iou: 0.7739900350570679, precision: 0.6795238268317492, recall: 0.7177184694254201, dice: 0.6980991087479459, acc: 0.6795238272755387\n",
      "inference: 56.70275115966797, batch_iou: 0.8613432049751282, precision: 0.7922148389806112, recall: 0.9311582071576222, dice: 0.8560855129412582, acc: 0.7922148396031902\n",
      "inference: 56.748191833496094, batch_iou: 0.8187260627746582, precision: 0.7392026007572491, recall: 0.8233938296219679, dice: 0.7790301429194003, acc: 0.7392026012626332\n",
      "inference: 56.8421745300293, batch_iou: 0.8564853668212891, precision: 0.841267019245485, recall: 0.9581905795436751, dice: 0.895930121663255, acc: 0.841267019520358\n",
      "inference: 56.82166290283203, batch_iou: 0.8279561400413513, precision: 0.8582874082297557, recall: 0.8695607515400468, dice: 0.8638873033910102, acc: 0.8582874086836807\n",
      "inference: 56.789825439453125, batch_iou: 0.9123630523681641, precision: 0.9079896697478362, recall: 0.9234340661509717, dice: 0.9156467466338751, acc: 0.9079896701306851\n",
      "inference: 56.72380828857422, batch_iou: 0.8959693908691406, precision: 0.8416584489620083, recall: 0.9119282637365211, dice: 0.875385428524973, acc: 0.8416584496352064\n",
      "inference: 56.91958236694336, batch_iou: 0.8261114358901978, precision: 0.8084262914241513, recall: 0.8131428322858457, dice: 0.8107777024683331, acc: 0.80842629190109\n",
      "inference: 56.79926300048828, batch_iou: 0.8790826201438904, precision: 0.8716713062449399, recall: 0.9522645472494589, dice: 0.9101873623315089, acc: 0.8716713065448681\n",
      "inference: 56.75843048095703, batch_iou: 0.8837357759475708, precision: 0.8509558085161107, recall: 0.9009774131735495, dice: 0.8752524965613123, acc: 0.8509558090546108\n",
      "inference: 56.94438552856445, batch_iou: 0.8022012114524841, precision: 0.7536640249002942, recall: 0.7703237272736355, dice: 0.7619028169757598, acc: 0.7536640254287952\n",
      "inference: 56.80748748779297, batch_iou: 0.8406193256378174, precision: 0.726990653676113, recall: 0.8234473126170182, dice: 0.7722185769597268, acc: 0.7269906542162398\n",
      "inference: 56.79478454589844, batch_iou: 0.8957964181900024, precision: 0.9040406248854189, recall: 0.917908379351433, dice: 0.9109217249372994, acc: 0.9040406251688539\n",
      "inference: 56.8218879699707, batch_iou: 0.9230517148971558, precision: 0.9328796530977347, recall: 0.9384656665817778, dice: 0.9356643226313829, acc: 0.932879653446105\n",
      "inference: 56.863487243652344, batch_iou: 0.921891450881958, precision: 0.9227035078850443, recall: 0.9097930909508923, dice: 0.9162028207751727, acc: 0.9227035081941024\n",
      "inference: 56.87654495239258, batch_iou: 0.8227270841598511, precision: 0.7693088359020199, recall: 0.7738811726955943, dice: 0.7715882305181654, acc: 0.7693088363317107\n",
      "inference: 56.814273834228516, batch_iou: 0.8435490131378174, precision: 0.7712602830751756, recall: 0.8600878825157221, dice: 0.8132557324912425, acc: 0.7712602836578264\n",
      "inference: 56.77596664428711, batch_iou: 0.8349423408508301, precision: 0.7174449956952157, recall: 0.8589192436055137, dice: 0.7818336620099356, acc: 0.7174449964157558\n",
      "inference: 56.7716178894043, batch_iou: 0.8953830003738403, precision: 0.8832074033253798, recall: 0.8658011028039907, dice: 0.8744176384357121, acc: 0.883207403925742\n",
      "inference: 56.9510383605957, batch_iou: 0.9160639047622681, precision: 0.888237192818575, recall: 0.9337862433895329, dice: 0.9104423741344332, acc: 0.8882371933704071\n",
      "inference: 56.76502227783203, batch_iou: 0.915911078453064, precision: 0.8997811723445681, recall: 0.9278656097497333, dice: 0.913607612014624, acc: 0.8997811728189071\n",
      "inference: 56.83555221557617, batch_iou: 0.8793853521347046, precision: 0.872730067884264, recall: 0.8982435117498734, dice: 0.8853030106965386, acc: 0.8727300683756247\n",
      "inference: 56.89686584472656, batch_iou: 0.9141655564308167, precision: 0.934107633575129, recall: 0.9446112251429198, dice: 0.9393300674695894, acc: 0.9341076338603338\n",
      "inference: 56.7658576965332, batch_iou: 0.9173678159713745, precision: 0.912938791079543, recall: 0.9217509042109494, dice: 0.9173236850901477, acc: 0.9129387917126109\n",
      "inference: 56.88185501098633, batch_iou: 0.8698021173477173, precision: 0.8780218614269266, recall: 0.820690338788968, dice: 0.8483886308357546, acc: 0.8780218622173661\n",
      "inference: 56.80137634277344, batch_iou: 0.9166103601455688, precision: 0.9123033937548445, recall: 0.9174418131868742, dice: 0.9148653884166594, acc: 0.9123033941485917\n",
      "inference: 56.92502212524414, batch_iou: 0.8500760197639465, precision: 0.8117171866759939, recall: 0.9007750838876993, dice: 0.8539304141279943, acc: 0.811717186977467\n",
      "inference: 56.825984954833984, batch_iou: 0.8845674991607666, precision: 0.8998765552440612, recall: 0.918233931145602, dice: 0.9089625663964293, acc: 0.8998765556103386\n",
      "inference: 56.793888092041016, batch_iou: 0.8866233825683594, precision: 0.8648763422010671, recall: 0.8907399311594354, dice: 0.8776176265453121, acc: 0.86487634280591\n",
      "inference: 56.79724884033203, batch_iou: 0.8978995680809021, precision: 0.903387875342507, recall: 0.8892559989101787, dice: 0.8962662344699495, acc: 0.9033878759302395\n",
      "inference: 56.7762565612793, batch_iou: 0.8757016658782959, precision: 0.8789661375288943, recall: 0.9326015933033582, dice: 0.9049898674501883, acc: 0.8789661378797838\n",
      "inference: 56.735809326171875, batch_iou: 0.8854628801345825, precision: 0.8703672500600438, recall: 0.843292160162435, dice: 0.8566158176062797, acc: 0.8703672507015575\n",
      "inference: 56.756065368652344, batch_iou: 0.8479902148246765, precision: 0.8036307972487152, recall: 0.8164300468860636, dice: 0.8099798619150521, acc: 0.8036307979088438\n",
      "inference: 56.7724494934082, batch_iou: 0.9227694272994995, precision: 0.942661224602068, recall: 0.9383587605614118, dice: 0.9405050720473014, acc: 0.9426612250276786\n",
      "inference: 56.73817443847656, batch_iou: 0.8884381055831909, precision: 0.8437453422733238, recall: 0.8999956877651877, dice: 0.8709632410936728, acc: 0.8437453428948454\n",
      "inference: 56.8966064453125, batch_iou: 0.8949058055877686, precision: 0.9030868428984649, recall: 0.8890655516482512, dice: 0.8960213479439003, acc: 0.9030868434158823\n",
      "inference: 56.87846374511719, batch_iou: 0.9243721961975098, precision: 0.938066223246608, recall: 0.9250453984387432, dice: 0.9315103111719701, acc: 0.9380662236306183\n",
      "inference: 56.77040100097656, batch_iou: 0.8859092593193054, precision: 0.9106208576074073, recall: 0.9065666663053344, dice: 0.9085892394323608, acc: 0.9106208580541002\n",
      "inference: 56.81353759765625, batch_iou: 0.8837621212005615, precision: 0.8260583627519117, recall: 0.9098483021556698, dice: 0.8659311171494016, acc: 0.8260583631048956\n",
      "inference: 56.887168884277344, batch_iou: 0.9221113324165344, precision: 0.9102756319882713, recall: 0.9428452750819553, dice: 0.9262742386046317, acc: 0.9102756325902104\n",
      "inference: 56.835777282714844, batch_iou: 0.9150575399398804, precision: 0.9162826918541785, recall: 0.9161759037077225, dice: 0.9162292946442178, acc: 0.9162826924042983\n",
      "inference: 56.78799819946289, batch_iou: 0.8735504150390625, precision: 0.8611954618953834, recall: 0.8434810125130672, dice: 0.8522461957300319, acc: 0.861195462429317\n",
      "inference: 56.846431732177734, batch_iou: 0.8664289712905884, precision: 0.874883007896947, recall: 0.8828865784337425, dice: 0.8788665719958155, acc: 0.874883008281081\n",
      "inference: 56.84975814819336, batch_iou: 0.8459389805793762, precision: 0.8103954477448451, recall: 0.8242761716502969, dice: 0.8172768759355105, acc: 0.8103954482839536\n",
      "inference: 56.90339279174805, batch_iou: 0.9208001494407654, precision: 0.9251693943444886, recall: 0.9325250123918971, dice: 0.9288326409163772, acc: 0.9251693947188857\n",
      "inference: 56.842430114746094, batch_iou: 0.8934577703475952, precision: 0.9111911823376382, recall: 0.8688410332258383, dice: 0.8895123148636523, acc: 0.9111911829002225\n",
      "inference: 56.853759765625, batch_iou: 0.8664954900741577, precision: 0.8245149361970328, recall: 0.8639730799285357, dice: 0.8437829608718124, acc: 0.824514936829048\n",
      "inference: 56.835201263427734, batch_iou: 0.8750805854797363, precision: 0.8374022728785093, recall: 0.8772400829713406, dice: 0.8568583842380331, acc: 0.8374022735525174\n",
      "inference: 56.857505798339844, batch_iou: 0.9288417100906372, precision: 0.9403880865570727, recall: 0.9431111548188832, dice: 0.9417476522408811, acc: 0.9403880868691934\n",
      "inference: 56.78220748901367, batch_iou: 0.851606011390686, precision: 0.7547406211197897, recall: 0.8560056934584606, dice: 0.8021899697534233, acc: 0.7547406218056137\n",
      "inference: 56.93062210083008, batch_iou: 0.9218367338180542, precision: 0.9177553445257329, recall: 0.9334124212188876, dice: 0.9255176695032281, acc: 0.9177553449114396\n",
      "inference: 56.74028778076172, batch_iou: 0.9095290899276733, precision: 0.9206959653511106, recall: 0.9247141066170479, dice: 0.922700661478266, acc: 0.9206959657049514\n",
      "inference: 56.92841720581055, batch_iou: 0.8842397332191467, precision: 0.8859798367145021, recall: 0.8531283569651811, dice: 0.8692438171637579, acc: 0.8859798372398812\n",
      "inference: 56.80192184448242, batch_iou: 0.9147889614105225, precision: 0.8947713630750305, recall: 0.9557713021093065, dice: 0.924265953823474, acc: 0.8947713633846459\n",
      "inference: 56.87433624267578, batch_iou: 0.9095028638839722, precision: 0.924906860845091, recall: 0.940764348462872, dice: 0.9327682133712218, acc: 0.9249068611262825\n",
      "inference: 56.832096099853516, batch_iou: 0.8951706886291504, precision: 0.9157649400367388, recall: 0.9040644258338407, dice: 0.9098770689408628, acc: 0.9157649404337161\n",
      "inference: 56.768287658691406, batch_iou: 0.8287353515625, precision: 0.8455615535560032, recall: 0.7147457119895366, dice: 0.7746698460024857, acc: 0.8455615542805576\n",
      "inference: 56.78636932373047, batch_iou: 0.8887212872505188, precision: 0.8632744585918898, recall: 0.8809689694947075, dice: 0.8720319628527126, acc: 0.8632744590995627\n",
      "inference: 56.81071853637695, batch_iou: 0.8592876195907593, precision: 0.9116281145640069, recall: 0.8819956758896486, dice: 0.8965671166099674, acc: 0.9116281149607296\n",
      "inference: 56.823936462402344, batch_iou: 0.8264076709747314, precision: 0.6983150591118239, recall: 0.8753642665204866, dice: 0.7768800663515405, acc: 0.698315059562277\n",
      "inference: 56.758846282958984, batch_iou: 0.7913623452186584, precision: 0.5862518150164138, recall: 0.8420626839569982, dice: 0.6912494091401318, acc: 0.5862518154067675\n",
      "inference: 56.79804611206055, batch_iou: 0.8479745388031006, precision: 0.8503713069973183, recall: 0.7798655823190257, dice: 0.8135938020011428, acc: 0.8503713075606887\n",
      "inference: 56.78927993774414, batch_iou: 0.8680644035339355, precision: 0.8068423293142933, recall: 0.8936316879346216, dice: 0.8480222164859376, acc: 0.8068423299101398\n",
      "inference: 56.95868682861328, batch_iou: 0.8736013770103455, precision: 0.8339785467121443, recall: 0.8722197633802342, dice: 0.852670602630548, acc: 0.8339785472392904\n",
      "inference: 56.92559814453125, batch_iou: 0.9381079077720642, precision: 0.946101620285258, recall: 0.9497945811942679, dice: 0.9479445040284916, acc: 0.9461016204976495\n",
      "inference: 56.757598876953125, batch_iou: 0.9016040563583374, precision: 0.9122401482992124, recall: 0.9220205699924562, dice: 0.9171042841433736, acc: 0.9122401486393019\n",
      "inference: 56.835777282714844, batch_iou: 0.8479318618774414, precision: 0.6779247754393694, recall: 0.8627074769303981, dice: 0.759234751310783, acc: 0.6779247759174457\n",
      "inference: 56.84979248046875, batch_iou: 0.8934042453765869, precision: 0.8562479318390537, recall: 0.9066626215351669, dice: 0.8807344116822813, acc: 0.8562479322824338\n",
      "inference: 56.864479064941406, batch_iou: 0.8775070309638977, precision: 0.8794516284295738, recall: 0.929672456158231, dice: 0.9038649835318399, acc: 0.8794516286848731\n",
      "inference: 56.85551834106445, batch_iou: 0.8191719055175781, precision: 0.5835110383617444, recall: 0.8803224408633782, dice: 0.7018255406443442, acc: 0.583511038700099\n",
      "inference: 56.73164749145508, batch_iou: 0.8367985486984253, precision: 0.8113131703687217, recall: 0.8097771089074502, dice: 0.8105444118053811, acc: 0.8113131711096991\n",
      "inference: 56.78799819946289, batch_iou: 0.8048056364059448, precision: 0.6937446342383388, recall: 0.8149993521953091, dice: 0.74949949430168, acc: 0.6937446346587346\n",
      "inference: 56.7836799621582, batch_iou: 0.9232074618339539, precision: 0.9140308203923938, recall: 0.9279635552745525, dice: 0.9209444946337286, acc: 0.9140308207277662\n",
      "inference: 56.832096099853516, batch_iou: 0.8690453767776489, precision: 0.8723872886597829, recall: 0.8391684218597849, dice: 0.8554554896696424, acc: 0.8723872894577075\n",
      "inference: 56.76291275024414, batch_iou: 0.815811038017273, precision: 0.8187337576700803, recall: 0.7471461743302593, dice: 0.7813035754032917, acc: 0.8187337584334432\n",
      "inference: 56.83184051513672, batch_iou: 0.8725634813308716, precision: 0.9211581106352856, recall: 0.9028843726132391, dice: 0.9119297060423471, acc: 0.9211581110927881\n",
      "inference: 56.73990249633789, batch_iou: 0.8582034707069397, precision: 0.8519978540260493, recall: 0.9108258431211557, dice: 0.8804302607910311, acc: 0.8519978544442742\n",
      "inference: 56.835487365722656, batch_iou: 0.8930458426475525, precision: 0.9196492026422819, recall: 0.9466547742535277, dice: 0.9329566020181137, acc: 0.9196492029599853\n",
      "inference: 56.80137634277344, batch_iou: 0.8418670892715454, precision: 0.7799146701125821, recall: 0.871537622612956, dice: 0.8231845150465295, acc: 0.7799146708819833\n",
      "inference: 56.78473663330078, batch_iou: 0.8288342356681824, precision: 0.7658639945271234, recall: 0.8228056801293075, dice: 0.7933143747878804, acc: 0.7658639950330587\n",
      "inference: 56.717472076416016, batch_iou: 0.9112106561660767, precision: 0.8913034821717406, recall: 0.9044124764242838, dice: 0.8978101304480299, acc: 0.8913034829274051\n",
      "inference: 56.80976104736328, batch_iou: 0.7835826277732849, precision: 0.668703372327971, recall: 0.6868033465816458, dice: 0.6776325155563225, acc: 0.6687033728787072\n",
      "inference: 56.778560638427734, batch_iou: 0.9164618849754333, precision: 0.9089356604055335, recall: 0.94192910224985, dice: 0.9251383114203364, acc: 0.9089356607524594\n",
      "inference: 56.7786865234375, batch_iou: 0.8764607906341553, precision: 0.8557994275142601, recall: 0.9323199055076343, dice: 0.8924223642456427, acc: 0.8557994280127748\n",
      "inference: 56.867454528808594, batch_iou: 0.8918724656105042, precision: 0.9004304979033516, recall: 0.9297558476326815, dice: 0.9148582305173415, acc: 0.9004304982537052\n",
      "inference: 56.82476806640625, batch_iou: 0.9106041193008423, precision: 0.9295554118740585, recall: 0.8801338718608566, dice: 0.9041698053843292, acc: 0.9295554123496446\n",
      "inference: 56.778079986572266, batch_iou: 0.9302284121513367, precision: 0.9369521275987474, recall: 0.9390565930766344, dice: 0.9380031799565621, acc: 0.9369521278854943\n",
      "inference: 56.70441436767578, batch_iou: 0.7886170744895935, precision: 0.70331316596122, recall: 0.7423054953416223, dice: 0.7222834651030436, acc: 0.7033131665576301\n",
      "inference: 56.84940719604492, batch_iou: 0.8141337633132935, precision: 0.771278865002122, recall: 0.7577748295056771, dice: 0.7644672158228569, acc: 0.7712788657479875\n",
      "inference: 56.812416076660156, batch_iou: 0.9089797735214233, precision: 0.9231121984858347, recall: 0.8885980836552595, dice: 0.9055263842549238, acc: 0.9231121989467879\n",
      "inference: 56.786495208740234, batch_iou: 0.8356475830078125, precision: 0.6438904064322866, recall: 0.885839030836937, dice: 0.7457308980898885, acc: 0.6438904067439916\n",
      "inference: 56.86307144165039, batch_iou: 0.8758641481399536, precision: 0.8849590549575488, recall: 0.8535014427973354, dice: 0.8689456345523682, acc: 0.8849590555890968\n",
      "inference: 56.787841796875, batch_iou: 0.9332762956619263, precision: 0.947512716238116, recall: 0.9432756865801133, dice: 0.9453894540626752, acc: 0.9475127165587279\n",
      "inference: 56.72796630859375, batch_iou: 0.8769845366477966, precision: 0.84548811149019, recall: 0.9210752176630629, dice: 0.8816645669539888, acc: 0.8454881118781837\n",
      "inference: 56.78403091430664, batch_iou: 0.8417684435844421, precision: 0.8734501611422502, recall: 0.7964143253558204, dice: 0.8331552965910055, acc: 0.8734501619375491\n",
      "inference: 56.86812973022461, batch_iou: 0.8477968573570251, precision: 0.8175844588556546, recall: 0.8691248718622236, dice: 0.8425672106849657, acc: 0.8175844592121652\n",
      "inference: 56.77827072143555, batch_iou: 0.8921290636062622, precision: 0.9104380737969204, recall: 0.896443011271314, dice: 0.9033863436497427, acc: 0.9104380743378737\n",
      "inference: 56.760353088378906, batch_iou: 0.8313363790512085, precision: 0.7974269939499684, recall: 0.7931225841066779, dice: 0.7952689645480855, acc: 0.7974269946990619\n",
      "inference: 56.919776916503906, batch_iou: 0.8929712176322937, precision: 0.856735707134129, recall: 0.8814816373156347, dice: 0.8689325258891073, acc: 0.8567357077698631\n",
      "inference: 56.7657585144043, batch_iou: 0.9353737235069275, precision: 0.9211031817011643, recall: 0.9472992383352108, dice: 0.9340175682560238, acc: 0.9211031820144536\n",
      "inference: 56.84291076660156, batch_iou: 0.8129628300666809, precision: 0.7472532348218783, recall: 0.8165373287822402, dice: 0.7803604579937522, acc: 0.747253235447998\n",
      "inference: 56.76063919067383, batch_iou: 0.929463267326355, precision: 0.9185715624286517, recall: 0.9342456318571233, dice: 0.9263422990452079, acc: 0.9185715628171579\n",
      "inference: 56.76278305053711, batch_iou: 0.885498046875, precision: 0.8964592892557414, recall: 0.9130734041746191, dice: 0.9046900759458746, acc: 0.8964592897022221\n",
      "inference: 56.73030471801758, batch_iou: 0.9146682024002075, precision: 0.9018331145018681, recall: 0.9304589527183816, dice: 0.9159224233312964, acc: 0.9018331148851391\n",
      "inference: 56.841217041015625, batch_iou: 0.8961024284362793, precision: 0.8691340938574007, recall: 0.8847129538291958, dice: 0.8768543327968985, acc: 0.8691340944432989\n",
      "inference: 56.779903411865234, batch_iou: 0.8834008574485779, precision: 0.9345132042810154, recall: 0.9121449307545295, dice: 0.9231935958459951, acc: 0.934513204740898\n",
      "inference: 56.77657699584961, batch_iou: 0.8885068893432617, precision: 0.883169641668381, recall: 0.8975008649028287, dice: 0.8902775828592144, acc: 0.8831696420959664\n",
      "inference: 56.79865646362305, batch_iou: 0.8586373925209045, precision: 0.8248148684069018, recall: 0.8370926160325209, dice: 0.8309083897352529, acc: 0.8248148689312353\n",
      "inference: 56.79302215576172, batch_iou: 0.8146159648895264, precision: 0.7697053697017826, recall: 0.7519543941854508, dice: 0.7607263445305869, acc: 0.7697053703416553\n",
      "inference: 56.80934524536133, batch_iou: 0.8498205542564392, precision: 0.853043615652733, recall: 0.8175678255759699, dice: 0.8349290525736376, acc: 0.8530436162695016\n",
      "inference: 16.757919311523438, batch_iou: 0.21306104958057404, precision: 0.8710361867780418, recall: 0.770929660849417, dice: 0.8179313019505, acc: 0.8710361905792846\n",
      "MEAN inference: 56.60277509689331, batch_iou: 0.8663958311080933, precision: 0.8459188223129863, recall: 0.8736673369059867, dice: 0.8578825849645517, acc: 0.8459188228339278\n"
     ]
    }
   ],
   "source": [
    "model.train(False)\n",
    "model.to(device)\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "jaccard = JaccardIndex(num_classes=2)\n",
    "mIoU = 0\n",
    "Precision = 0\n",
    "Recall = 0 \n",
    "Dice = 0\n",
    "Pixel_accuracy = 0 \n",
    "time = 0\n",
    "metric_calculator = metrics.SegmentationMetrics(average=True, ignore_background=True, activation=\"softmax\")\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        tinputs, tlabels = data\n",
    "        tinputs = torch.tensor(tinputs).permute(0,3,1,2).to(device=device, dtype=torch.float)\n",
    "        tlabels = torch.tensor(tlabels).to(device=device, dtype=torch.long)\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        starter.record()\n",
    "        toutputs = model(tinputs)\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        batch_iou = 0\n",
    "        for i in range(len(toutputs)):\n",
    "            pred = F.softmax(toutputs, dim=1)[i].permute(1,2,0)[:,:, 1]\n",
    "            batch_iou += jaccard(pred.cpu(), tlabels[i].cpu().int())\n",
    "        mIoU += batch_iou/batch_size\n",
    "        time += curr_time\n",
    "        pixel_accuracy, dice, precision, recall = metric_calculator(tlabels.int(), toutputs)\n",
    "        Pixel_accuracy += pixel_accuracy\n",
    "        Dice += dice\n",
    "        Recall += recall\n",
    "        Precision += precision\n",
    "        print(\"inference: {0}, batch_iou: {1}, precision: {2}, recall: {3}, dice: {4}, acc: {5}\".format(curr_time, batch_iou/batch_size, precision, recall, dice, pixel_accuracy))\n",
    "mIoU /= len(test_loader)\n",
    "mTime = time / len(test_loader)\n",
    "Precision /= len(test_loader)\n",
    "Dice /= len(test_loader)\n",
    "Recall /= len(test_loader)\n",
    "Pixel_accuracy /= len(test_loader)\n",
    "# print(\"mean inference: {0}, mean iou: {1}\".format( mTime, mIoU))\n",
    "print(\"MEAN inference: {0}, batch_iou: {1}, precision: {2}, recall: {3}, dice: {4}, acc: {5}\".format(mTime, mIoU, Precision, Recall, Dice, Pixel_accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
